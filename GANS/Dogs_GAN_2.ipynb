{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, LeakyReLU, UpSampling2D, Reshape, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.callbacks import Callback"
      ],
      "metadata": {
        "id": "s5CLe5sQ6am5",
        "collapsed": true
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load dataset\n",
        "ds, info = tfds.load('stanford_dogs', split='train', with_info=True)\n"
      ],
      "metadata": {
        "id": "1VPJRlfj6dgN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a function to scale images\n",
        "def scale_image(item):\n",
        "    image = item['image']\n",
        "    image = tf.image.resize(image, (64, 64))\n",
        "    return tf.cast(image, tf.float32) / 255.0, item['label']\n"
      ],
      "metadata": {
        "id": "8PMKqIRt6fzJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocess dataset\n",
        "ds = ds.map(scale_image)\n",
        "ds = ds.cache()\n",
        "ds = ds.shuffle(60000)\n",
        "ds = ds.batch(batch_size=16)\n",
        "ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "68dQmN4E6iZ2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generator model\n",
        "def build_gen():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16*16*128, input_dim=128))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Reshape((16, 16, 128)))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Conv2D(3, kernel_size=4, padding=\"same\", activation=\"sigmoid\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "x6NnU6LG6kkj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Discriminator model\n",
        "def build_disc():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, kernel_size=5, input_shape=(64,64,3), padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(128, kernel_size=5, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(256, kernel_size=5, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Conv2D(512, kernel_size=5, padding='same'))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation=\"sigmoid\"))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "JVenqMtC6nRq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# GAN class\n",
        "class GAN(Model):\n",
        "    def __init__(self, Generator, discriminator, *args , **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.generator = Generator\n",
        "        self.discriminator = discriminator\n",
        "\n",
        "    def compile(self, g_opt, d_opt , g_loss, d_loss, *args, **kwargs):\n",
        "        super().compile(*args, **kwargs)\n",
        "        self.g_opt = g_opt\n",
        "        self.d_opt = d_opt\n",
        "        self.g_loss = g_loss\n",
        "        self.d_loss = d_loss\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        real_images, _ = batch_data\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        noise = tf.random.normal((batch_size, 128))\n",
        "\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            fake_images = self.generator(noise, training=False)\n",
        "\n",
        "            # Add noise to real images\n",
        "            real_images_with_noise = real_images + tf.random.normal(tf.shape(real_images), mean=0.0, stddev=0.1)\n",
        "\n",
        "            yhat_real = self.discriminator(real_images_with_noise, training=True)  # Use noisy real images\n",
        "            yhat_fake = self.discriminator(fake_images, training=True)\n",
        "            yhat_real = self.discriminator(real_images, training=True)\n",
        "            yhat_fake = self.discriminator(fake_images, training=True)\n",
        "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
        "            y_realfake_labels = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
        "            d_loss_value = self.d_loss(y_realfake_labels, yhat_realfake)\n",
        "\n",
        "        d_gradient = d_tape.gradient(d_loss_value, self.discriminator.trainable_variables)\n",
        "        self.d_opt.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
        "\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            gen_images = self.generator(noise, training=True)\n",
        "            predicted_labels = self.discriminator(gen_images, training=False)\n",
        "            g_loss_value = self.g_loss(tf.ones_like(predicted_labels), predicted_labels)\n",
        "\n",
        "        g_gradient = g_tape.gradient(g_loss_value, self.generator.trainable_variables)\n",
        "        self.g_opt.apply_gradients(zip(g_gradient, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss_value, \"g_loss\": g_loss_value}\n"
      ],
      "metadata": {
        "id": "ueA_Qh1G6sOw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models and optimizer\n",
        "Generator = build_gen()\n",
        "discriminator = build_disc()\n",
        "g_opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "d_opt = Adam(learning_rate=0.00002, beta_1=0.5)\n",
        "g_loss = BinaryCrossentropy(from_logits=True)\n",
        "d_loss = BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Compile GAN model\n",
        "gan = GAN(Generator, discriminator)\n",
        "gan.compile(g_opt, d_opt, g_loss, d_loss)\n",
        "\n",
        "# Callback for generating images\n",
        "class ModelMoniter(Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images = generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = array_to_img(generated_images[i])\n",
        "            img.save(f\"generated_img{i}_{epoch}.png\")\n"
      ],
      "metadata": {
        "id": "GBMeQUvog7lM",
        "outputId": "4b1485fc-fa78-488f-e869-5523c837e6f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVgvhH6F5U4V",
        "outputId": "30d6bdc5-457c-44ec-9438-49eabd8aa5e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:681: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m627/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1:46\u001b[0m 865ms/step - d_loss: 0.5104 - g_loss: 0.0534"
          ]
        }
      ],
      "source": [
        "hist = gan.fit(ds, epochs=1, callbacks=[ModelMoniter()])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Generate a new image using the trained generator\n",
        "def generate_and_display_image(Generator, latent_dim=128):\n",
        "    random_latent_vector = tf.random.normal(shape=(1, 128))\n",
        "    generated_image = Generator(random_latent_vector, training=False)\n",
        "    generated_image = np.squeeze(generated_image.numpy())  # Remove batch dimension\n",
        "\n",
        "    plt.imshow(generated_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Generate and display a new image\n",
        "generate_and_display_image(Generator)"
      ],
      "metadata": {
        "id": "sD8gLGP_M2BN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}